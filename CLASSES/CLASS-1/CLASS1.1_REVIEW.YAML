1) SPARK INTRODUCTION:
    - it is a data processing tool used for big data.
    - it is distributed in cluster in memory. Cluster is a group of computers which are connected to each other for sharing resources with same objective to work together.
    - it is faster than hadoop and scalable.
    - it is used for real time processing
    - data in hdfs or cloud storage can be processed using spark.
    - the data is copied between nodes in cluster for processing.
    - partition is a logical division of data in spark for processing in parallel, having each particioned data copys in different cluster.
    - spark is written in scala, but can be used in python, java, R, SQL.
  
2) SPARK COMPONENTS and ARCHITECTURE:
    - SPARK CORE: it is the base engine for large scale parallel and distributed data processing.
    - SPARK SQL: it is used for structured data processing.
    - SPARK STREAMING: it is used for real time data processing.
    - MLlib: it is used for machine learning.
    - GRAPHX: it is used for graph processing.
    - SPARK ARCHITECTURE: it has driver program which is the main program and worker nodes which are the nodes in cluster where the data is processed.
    - TUNGSTEN: it is the memory management module in spark.
    - ESTRUCTURY OF SPARK BY STEP:
        - STEP 1: create a driver program with spark context. it requests resources from cluster manager, turnning the operation in dags and sending to worker nodes to executor
        - STEP 2: cluster manager is used to create worker nodes and manage resources such as yarn, mesos, standalone, kubernetes.
        - STEP 3: executor is created in worker nodes to execute the tasks.
    - TRANSFORMATION AND ACTION: transformation is the operation on data which is not executed until an action is called. Action is the operation on data which is executed. EACH TRANSFORMATION CREATES A NEW RDD(RESILIENT DISTRIBUTED DATASET - DATAFRAME).
    - LIST OF TRANSFORMATIONS AND ACTION:
            1) MAP: it is used to apply a function to each element in RDD.
            2) FILTER: it is used to filter the elements in RDD.
            3) FLATMAP: it is used to apply a function to each element in RDD and return a new RDD.
            4) GROUPBYKEY: it is used to group the elements in RDD by key.
            5) REDUCEBYKEY: it is used to reduce the elements in RDD by key.
            6) JOIN: it is used to join two RDDs.
            7) COGROUP: it is used to group the elements in RDD by key.
            8) CARTESIAN: it is used to find the cartesian product of two RDDs.
            9) UNION: it is used to find the union of two RDDs.
            10) DISTINCT: it is used to find the distinct elements in RDD.
            11) SAMPLE: it is used to find the sample of elements in RDD.
            12) SORTBYKEY: it is used to sort the elements in RDD by key.
            13) SUBTRACT: it is used to subtract the elements in RDD.
            14) INTERSECTION: it is used to find the intersection of two RDDs.
            15) COALESCE: it is used to reduce the number of partitions in RDD.
            16) REPARTITION: it is used to increase the number of partitions in RDD.
            17) COLLECT: it is used to collect the elements in RDD.
            18) COUNT: it is used to count the elements in RDD.
            19) FIRST: it is used to find the first element in RDD.
            20) TAKE: it is used to take the first n elements in RDD.
            21) TAKEORDERED: it is used to take the first n elements in RDD in order.
            22) SAVEASTEXTFILE: it is used to save the RDD as text file.
            23) SAVEASSEQUENCEFILE: it is used to save the RDD as sequence file.
    - COMPONENTS OF SPARK:
            1) TAKS: it is the unit of work in spark.
            2) STAGE: it is a group of tasks.
            3) JOB: it is a group of stages.
            - OVERVIEW: a driver creates a job, job creates stages, stages creates tasks, tasks are executed in worker nodes particioned.
      
              
3) Context e Session with examples: 
    - CONTEXT: it is the entry point to spark core functionality. it is used to create RDD, accumulators, broadcast variables, etc.
    - SESSION: it is the entry point to spark sql functionality. it is used to create dataframes, datasets, sql tables, etc.
    - EXAMPLES:
        - CONTEXT:
            - from pyspark import SparkContext
            - sc = SparkContext()
            - data = [1, 2, 3, 4, 5]
            - rdd = sc.parallelize(data)
            - rdd.collect()
        - SESSION:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - data = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]
            - df = spark.createDataFrame(data, ['id', 'value'])
            - df.show()

4) BIG DATA FORMATS WITH EXAMPLE:
    - AVRO: it is a row oriented format. it is used for serialization and deserialization of data. it is used for data exchange between systems.
    - PARQUET: it is a column oriented format. it is used for storing data in columnar format. it is used for data processing. BETTER FOR READ
    - ORC: it is a column oriented format. it is used for storing data in columnar format. it is used for data processing. BETTER FOR WRITE
    - EXAMPLES:
        - AVRO:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - df = spark.read.format('avro').load('file.avro')
            - df.show()
        - PARQUET:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - df = spark.read.format('parquet').load('file.parquet')
            - df.show()
        - ORC:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - df = spark.read.format('orc').load('file.orc')
            - df.show()
        

5) Using Putty com SSH by host windowns:
    - download putty in host windows
    - sudo apt install openssh-server in ghest ubuntu
    - sudo service ssh start in ghest ubuntu
    - setting port of my virtual machine. I need to change the port at vitual box settings.
        Network -> adapter 1 -> advanced -> port forwarding -> port forwarding rules -> add new rule
            name = ssh, protocol = tcp, host port = 22 guest port = 22
    - restart the virtual machine and keep it on
    - open putty and put the ip of the virtual machine and port 22
    - login with the user and password of the virtual machine
    - at terminal opeend by putty, I can run the commands of the virtual machine.
    - pyspark, at terminal, to open the pyspark shell.

6) RDD, Dataset e Dataframe with example. they are structures of data in spark:
    - RDD: 
           it is the base abstraction in spark. 
           it is a distributed collection of objects. 
           it is immutable and fault tolerant. 
           it is used for low level api.
           it is more hard to use than dataset and dataframe.
           it is the based structure of spark.
    - DATASET: 
           it is a distributed collection of objects. 
           it is immutable and fault tolerant. 
           it is used for high level api.
           it is like database table.
           it is usable only in scala and java.
    - DATAFRAME: 
           it is a distributed collection of objects. 
           it is immutable and fault tolerant. 
           it is used for high level api.
           it is like database table with schema.
           it is more usable for data science.
           
    - EXAMPLES:
        - RDD:
            - from pyspark import SparkContext
            - sc = SparkContext()
            - data = [1, 2, 3, 4, 5]
            - rdd = sc.parallelize(data)
            - rdd.collect()
        - DATASET:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - data = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]
            - ds = spark.createDataset(data)
            - ds.show()
        - DATAFRAME:
            - from pyspark.sql import SparkSession
            - spark = SparkSession.builder.appName('example').getOrCreate()
            - data = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]
            - df = spark.createDataFrame(data, ['id', 'value'])
            - df.show()


7) RDD with example:
    data types supported by rdd is: int, float, string, list, tuple, dictionary, set, etc.
    numbers = sc.parallelize([1,2,3,4,5,6,7,8,9,10]) -> to create an RDD
    numbers.collect() -> to show the elements of the RDD
    numbers.count() -> to count the elements of the RDD
    numbers.countByValue() -> to count the elements of the RDD
    numbers.first() -> to show the first element of the RDD
    numbers.take(3) -> to show the first n elements of the RDD
    numbers.top(3) -> to show the last n elements of the RDD
    numbers.mean() -> to calculate the mean of the elements of the RDD
    numbers.takeOrdered(3) -> to show the first n elements of the RDD in order
    numbers.reduce(lambda x, y: x + y) -> to sum the elements of the RDD
    numbers.filter(lambda x: x % 2 == 0) -> to filter the elements of the RDD
    numbers.map(lambda x: x * 2) -> to apply a function to each element of the RDD
    numbers.flatMap(lambda x: [x, x * 2]) -> to apply a function to each element of the RDD and return a new RDD
    numbers.groupBy(lambda x: x % 2).collect() -> to group the elements of the RDD by key
    numbers.groupByKey().mapValues(list).collect() -> to group the elements of the RDD by key
    numbers.sortBy(lambda x: x, ascending=False).collect() -> to sort the elements of the RDD by key
    numbers.distinct().collect() -> to find the distinct elements of the RDD
    numbers.sample(True, 0.5, 1).collect() -> to find the sample of elements of the RDD
    numbers.union(numbers).collect() -> to find the union of two RDDs
    numbers.intersection(numbers).collect() -> to find the intersection of two RDDs
    numbers.subtract(numbers).collect() -> to subtract the elements of the RDD
    numbers.max() -> to find the maximum element of the RDD
    numbers.cartesian(numbers).collect() -> to find the cartesian product of two RDDs
    numbers.coalesce(1).collect() -> to reduce the number of partitions of the RDD
    numbers.repartition(2).collect() -> to increase the number of partitions of the RDD
    numbers.saveAsTextFile('numbers.txt') -> to save the RDD as text file
    numbers.saveAsSequenceFile('numbers.seq') -> to save the RDD as sequence file
    
    purchases = sc.parallelize([(1, 'apple', 1.0), (2, 'orange', 2.0), (3, 'banana', 3.0)])
    keys = purchases.keys()
    values = purchases.values()
    purchases.countByKey()
    sums = purchases.mapValues(lambda x: x + 2)
    valuesbykey = purchases.groupByKey().mapValues(list)
    purchases2 = sc.parallelize([(1, 400), (2, 500), (3, 600)])
    debits = sc.parallelize([(1, 20), (2, 40), (3, 60)])
    credits = sc.parallelize([(1, 10), (2, 20), (3, 30)])
    join_debits_credits = debits.join(credits)

8) Dataframe with example:
    obs: 
        the schema can be created by own spark or by myself.
        the schema is the structure of the dataframe.

    - EXAMPLES:
        - from pyspark.sql import SparkSession
        - spark = SparkSession.builder.appName('example').getOrCreate() -> to create a spark session
        - schema = "id INT, name STRING " -> to create a schema
        - datas = [(1, 'John'), (2, 'Jane'), (3, 'Doe')] -> to create a data
        - df = spark.createDataFrame(datas, schema) -> to create a dataframe
        - df.show() -> to show the dataframe
        - df.printSchema()
        - df.schema
        - df.columns
        - df.orderBy('id', ascending=False).show()
        - df.describe().show()
        - df.select('id').show()
        - df.filter(df['id'] > 2).show()
        - df.groupBy('id').count().show()
        - df.agg({'id': 'max'}).show()
        - df.withColumn('id2', df['id'] * 2).show()
        - df.withColumnRenamed('id', 'id2').show()
        - df.drop('id').show()
        - df.sort('id', ascending=False).show()
        - df.join(df, df['id'] == df['id'], 'inner').show()
        - df.union(df).show()
        - df.distinct().show()
        - df.sample(False, 0.5, 1).show()
        - df.repartition(2).show()
        - df.coalesce(1).show()
        - df.write.format('csv').save('data.csv')
        - df.write.format('json').save('data.json')
        - df.write.format('parquet').save('data.parquet')
        - df.write.format('orc').save('data.orc')
        - df.write.format('avro').save('data.avro')
        - df.write.format('jdbc').save('data.jdbc')
        - df.write.format('text').save('data.txt')
        - df.write.format('libsvm').save('data.libsvm')
        - df.write.format('kafka').save('data.kafka')
        - df.write.format('console').save('data.console')
        - df.write.format('memory').save('data.memory

        - from pyspark.sql.functions import sum, avg, count, max, min, mean, stddev, variance, expr
        - schema2 = "Product STRING , Category STRING, sales INT"
        - datas2 = [("apple", "fruit", 1), ("orange", "fruit", 2), ("banana", "fruit", 3)]
        - df2 = spark.createDataFrame(datas2, schema2)
        - df2.show()
        - df2.select('Product').show() -> to select a column
        - df2.select('Product', 'sales', expr('sales * 2').alias('new_sales')).show() -> to select a column and apply a function
        - despachantes.groupBy('nome') \
            .agg(Func.sum('vendas').alias('total_sales')) \
            .orderBy(Func.col('total_sales').desc()) \
            .show()
9) importing data to spark:
    - from pyspark.sql import SparkSession
    - from pyspark.sql.types import *
    - spark = SparkSession.builder.appName('example').getOrCreate()
    - df = spark.read.format('csv').option('header', 'true').load('data.csv')
    - df = spark.read.format('json').load('data.json')
    - df = spark.read.format('parquet').load('data.parquet') -> format more analitycs
    - df = spark.read.format('orc').load('data.orc') -> format more analitycs
    - df = spark.read.format('avro').load('data.avro')
    - df = spark.read.format('jdbc').load('data.jdbc')
    - df = spark.read.format('text').load('data.txt')
    - df = spark.read.format('libsvm').load('data.libsvm')
    - df = spark.read.format('kafka').load('data.kafka')
    - df = spark.read.format('console').load('data.console')
    - df = spark.read.format('memory').load('data.memory')

    - schema = 'id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING'
    - despachantes = spark.read.csv('/home/tonycastellamare/Desktop/COURSES/FormaçãoSparkcomPyspark:oCursoCompleto/despachantes.csv', header=False, schema = schema)
    - despachante_auto_schema = spark.read.load(/home/tonycastellamare/Desktop/COURSES/FormaçãoSparkcomPyspark:oCursoCompleto/despachantes.csv', header=False, format='csv', sep=',', inferSchema=True)

    - from pyspark.sql import functions as Func
    - despachentes.select('id', 'nome', 'vendas').where((Func.col('vendas') > 1) & (Func.col('vendas') < 3)).show()
    - new_despachantes = despachantes.withColumnRenamed('vendas', 'vendas_total').show()
    - despachantes2 = despachantes.withColumn('data', Func.to_timestamp(Func.col('data'), 'yyyy-MM-dd')).show()
    - despachantes.select(year('data')).show()
    - despachantes.select(month('data')).show()
    - despachantes.orderBy(col('vendas').desc(), col("cidade").desc()).show()
    - despachantes.filter(Func.col('vendas') > 50).show()

10) exporting data from spark:
    - df.write.format('csv').save('/home/course/export_datas/data.csv')
    - df.write.format('json').save('data.json')
    - df.write.format('parquet').save('data.parquet')
    - df.write.format('orc').save('data.orc')
    - df.write.format('avro').save('data.avro')
    - df.write.format('jdbc').save('data.jdbc')
    - df.write.format('text').save('data.txt')
    - df.write.format('libsvm').save('data.libsvm')
    - df.write.format('kafka').save('data.kafka')
    - df.write.format('console').save('data.console')
    - df.write.format('memory').save('data.memory')

11) to save a dataframe as a table in spark:
    - df.write.saveAsTable('table_name')
    - df.write.saveAsTable('table_name', format='parquet', mode='overwrite', partitionBy='column_name')
    - df.write.saveAsTable('table_name', format='parquet', mode='overwrite', partitionBy='column_name', bucketBy=10, sortBy='column_name')
    - df.write.saveAsTable('table_name', format='parquet', mode='overwrite', partitionBy='column_name', bucketBy=10, sortBy='column_name', location='path')
    - df.write.saveAsTable('table_name', format='parquet', mode='overwrite', partitionBy='column_name', bucketBy=10, sortBy='column_name', location='path', properties={'key': 'value'})
    - despachante.write.option("path", "path").saveAsTable("table_name", mode="overwrite") -> to create a external table