1) USING SPARK IN JUBYTER NOTE:
    pip install jupyter
    pip install findspark # to find the spark in the system

    # to start the jupyter notebook
    jupyter notebook

    # to start the pyspark in jupyter notebook
    import findspark # to import the findspark
    findspark.init() # to find the spark in the system
    import pyspark # to import the pyspark
    from pyspark.sql import SparkSession ...
    

2) TURN DATAFRAME FROM PANDAS TO SPARK:
    import pandas as pd

    churn = pd.read_csv('/home/tonycastellamare/Desktop/Spark/download/Churn.csv', sep=';')
    churn.head()

    # we need to create a context variable of spark
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName('churn').getOrCreate()

    # to turn the dataframe from pandas to spark
    churn_spark = spark.createDataFrame(churn) # to turn the dataframe from pandas to spark
    churn_spark.show(5)

    # to turn the dataframe from spark to pandas
    churn_pd = churn_spark.toPandas()
    churn_pd.head()

3) USING THE KOALAS LIBRARY TO NOT MAKE MODIFICATIONS IN THE CODE:
    # to install the koalas library
    pip install koalas

    import databricks.koalas as ks

    # dataframe from csv
    churn = ks.read_csv('/home/tonycastellamare/Desktop/Spark/download/Churn.csv', sep=';')
    churn.head()

    # to turn the dataframe from pandas to spark
    churn_koalas = ks.from_pandas(churn)
    churn_koalas.head()

    # to turn the dataframe from spark to pandas
    churn_pd = churn_koalas.to_pandas()
    churn_pd.head()


4) SPARK UI:
    how to access SPARK UI:
        -  http://localhost:4040/jobs/ # to access we need inialize the spark on terminal first


5) SPARK CLUSTER OVERVIEW TO CREATE MASTER AND WORKER:
    # to create a master
    cd /opt/spark/conf/
    cp spark-env.sh.template spark-env.sh
    nano spark-env.sh
    spark-env.sh:
     SPARK_MASTER_HOST='192.168.56.101' # ip of the master machine to create the cluster
    
    cd /opt/spark/sbin/
    ./start-master.sh # to start the master

    # access the 10.0.2.15:8080 to see the master -> localhost is the ip of the master machine

    # to create a worker
    ./start-slave.sh spark://192.168.56.101:7077 # localhost is the ip of the master machine

    # setting of the slaves:
    cd /opt/spark/conf/
    cp spark-env.sh.template spark-env.sh
    nano spark-env.sh
    spark-env.sh:
        SPARK_MASTER_HOST='192.168.56.101'
    
    cd /opt/spark/sbin/
    ./start-slave.sh spark://192.168.56.101:7077 # 192.168.. is the master machine ip and 7077 is the port of the master that it here will access there


6) EXECUTING THE SPARK IN THE CLUSTER:
    # we need to manage the spark in the cluster. some are standalone, yarn, mesos
    pyspark --master spark://192.168.56.101:7077 # now we are executing the spark in the cluster

